{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MovieRatings\").getOrCreate()\n",
    "\n",
    "def get_rating_data():\n",
    "    schema = StructType([\n",
    "        StructField(\"userId\", IntegerType(), True),\n",
    "        StructField(\"movieId\", IntegerType(), True),\n",
    "        StructField(\"rating\", FloatType(), True),\n",
    "        StructField(\"timestamp\", IntegerType(), True)\n",
    "    ])\n",
    "    data = spark.read.csv(os.getenv('BASE_PROJECT_PATH') + 'data/u.data', sep='\\t', schema=schema, header=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.9157990063912332\n",
      "R Squared (R2) on test data = 0.33969955154459863\n",
      "Mean Squared Error (MSE) on test data = 0.8386878201071701\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "# Load the data\n",
    "data = get_rating_data()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(training, test) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Build the ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [20, 30, 40]) \\\n",
    "    .addGrid(als.maxIter, [10, 15, 20]) \\\n",
    "    .addGrid(als.regParam, [0.05, 0.1, 0.2]) \\\n",
    "    .addGrid(als.alpha, [1.0, 5.0, 10.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define an evaluators \n",
    "evaluator_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_r2 = RegressionEvaluator(metricName=\"r2\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_mse = RegressionEvaluator(metricName=\"mse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_accuracy = RegressionEvaluator(metricName=\"accuracy\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Use TrainValidationSplit to choose the best combination of parameters\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                           estimatorParamMaps=param_grid,\n",
    "                           evaluator=evaluator_rmse,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Train the model\n",
    "model = tvs.fit(training)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "mse = evaluator_mse.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)\n",
    "print(\"R Squared (R2) on test data =\", r2)\n",
    "print(\"Mean Squared Error (MSE) on test data =\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a new path\n",
    "model.bestModel.write().save(os.getenv('BASE_PROJECT_PATH') + 'recommendation/best_model_als')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                                      |\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1     |[{114, 5.0185556}, {408, 4.9068847}, {169, 4.83205}, {1449, 4.8184934}, {285, 4.8016534}, {50, 4.7903686}, {647, 4.7859}, {223, 4.7706037}, {963, 4.738607}, {48, 4.724719}]         |\n",
      "|2     |[{1643, 4.866243}, {1449, 4.8260517}, {483, 4.7573133}, {694, 4.691554}, {127, 4.680705}, {1194, 4.6368113}, {169, 4.6052384}, {285, 4.5930123}, {50, 4.5888834}, {178, 4.5639105}]  |\n",
      "|3     |[{320, 4.6748524}, {340, 4.049389}, {346, 3.954154}, {198, 3.8701317}, {318, 3.8522155}, {902, 3.8499022}, {1643, 3.820812}, {357, 3.81159}, {321, 3.7572439}, {132, 3.756666}]      |\n",
      "|4     |[{1368, 5.3835053}, {1137, 5.3011923}, {1643, 5.097612}, {320, 4.994603}, {922, 4.9810143}, {813, 4.973256}, {1169, 4.969084}, {318, 4.965376}, {1005, 4.962752}, {169, 4.954206}]   |\n",
      "|5     |[{408, 4.48885}, {50, 4.430817}, {390, 4.401334}, {169, 4.3022966}, {434, 4.2374864}, {172, 4.2253966}, {114, 4.080207}, {174, 4.05866}, {302, 4.058083}, {168, 4.0474753}]          |\n",
      "|6     |[{474, 4.5945663}, {1449, 4.4407043}, {178, 4.4327974}, {603, 4.4006004}, {1142, 4.3744516}, {483, 4.3691797}, {197, 4.353527}, {641, 4.3385577}, {657, 4.3287134}, {1203, 4.327884}]|\n",
      "|7     |[{318, 5.0176563}, {50, 4.926911}, {64, 4.8724217}, {199, 4.844598}, {166, 4.813371}, {626, 4.800886}, {633, 4.7896233}, {127, 4.7886677}, {528, 4.782015}, {498, 4.78097}]          |\n",
      "|8     |[{313, 4.938265}, {50, 4.901528}, {64, 4.8649898}, {127, 4.8501267}, {357, 4.7914696}, {98, 4.7777915}, {185, 4.7381096}, {172, 4.7030253}, {318, 4.6939807}, {483, 4.684712}]       |\n",
      "|9     |[{1169, 4.986664}, {192, 4.9054117}, {127, 4.882436}, {318, 4.836978}, {481, 4.831261}, {483, 4.825168}, {588, 4.8144574}, {187, 4.779681}, {200, 4.66303}, {1398, 4.656397}]        |\n",
      "|10    |[{1449, 4.920345}, {1643, 4.872052}, {483, 4.866349}, {127, 4.8573713}, {318, 4.8046007}, {64, 4.7525735}, {134, 4.715099}, {1398, 4.7090073}, {603, 4.695647}, {178, 4.6921906}]    |\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "model1 = ALSModel.load(os.getenv('BASE_PROJECT_PATH') + 'recommendation/best_model_als')\n",
    "\n",
    "# Make recommendations for users\n",
    "userRecs = model1.recommendForAllUsers(10)\n",
    "\n",
    "print(userRecs.show(10, False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
